{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Low Noise Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from nltk.corpus import stopwords\n",
    "import pprint as pp\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "try:\n",
    "   import cPickle as cPickle\n",
    "except:\n",
    "   import pickle as cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/processed_pos_neg_reviews.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B006CMVE7S</td>\n",
       "      <td>4</td>\n",
       "      <td>No taste with filtered bottle</td>\n",
       "      <td>I guess some of you may have guessed this befo...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00570H26I</td>\n",
       "      <td>4</td>\n",
       "      <td>Delicious pasta,  but not for peanut allergies!</td>\n",
       "      <td>I have to agree with the previous posters that...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000GBOM0C</td>\n",
       "      <td>5</td>\n",
       "      <td>great treat</td>\n",
       "      <td>My pups love this chicken/rice treat(10lb Russ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId  Rating                                          Summary  \\\n",
       "0  B006CMVE7S       4                    No taste with filtered bottle   \n",
       "1  B00813GRG4       1                                Not as Advertised   \n",
       "2  B00570H26I       4  Delicious pasta,  but not for peanut allergies!   \n",
       "3  B000UA0QIQ       2                                   Cough Medicine   \n",
       "4  B000GBOM0C       5                                      great treat   \n",
       "\n",
       "                                                Text     Label  \n",
       "0  I guess some of you may have guessed this befo...  POSITIVE  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  NEGATIVE  \n",
       "2  I have to agree with the previous posters that...  POSITIVE  \n",
       "3  If you are looking for the secret ingredient i...  NEGATIVE  \n",
       "4  My pups love this chicken/rice treat(10lb Russ...  POSITIVE  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POSITIVE    82037\n",
       "NEGATIVE    82037\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B006CMVE7S</td>\n",
       "      <td>4</td>\n",
       "      <td>No taste with filtered bottle</td>\n",
       "      <td>I guess some of you may have guessed this befo...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00570H26I</td>\n",
       "      <td>4</td>\n",
       "      <td>Delicious pasta,  but not for peanut allergies!</td>\n",
       "      <td>I have to agree with the previous posters that...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000GBOM0C</td>\n",
       "      <td>5</td>\n",
       "      <td>great treat</td>\n",
       "      <td>My pups love this chicken/rice treat(10lb Russ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId  Rating                                          Summary  \\\n",
       "0  B006CMVE7S       4                    No taste with filtered bottle   \n",
       "1  B00813GRG4       1                                Not as Advertised   \n",
       "2  B00570H26I       4  Delicious pasta,  but not for peanut allergies!   \n",
       "3  B000UA0QIQ       2                                   Cough Medicine   \n",
       "4  B000GBOM0C       5                                      great treat   \n",
       "\n",
       "                                                Text     Label  \n",
       "0  I guess some of you may have guessed this befo...  POSITIVE  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  NEGATIVE  \n",
       "2  I have to agree with the previous posters that...  POSITIVE  \n",
       "3  If you are looking for the secret ingredient i...  NEGATIVE  \n",
       "4  My pups love this chicken/rice treat(10lb Russ...  POSITIVE  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 70000 reviews\n",
    "training_data = data[0:70000].reset_index(drop=True)\n",
    "training_data_length = training_data.shape[0]\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 5)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(training_data.shape)\n",
    "pp.pprint(training_data.Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001D0KG3W</td>\n",
       "      <td>5</td>\n",
       "      <td>Italian blend coffee</td>\n",
       "      <td>Great coffee - much better price when you orde...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001VJ0B0I</td>\n",
       "      <td>2</td>\n",
       "      <td>Gross by-products, sugar, and food colorings--...</td>\n",
       "      <td>I completely agree that the ingredients for th...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0005XMOI8</td>\n",
       "      <td>5</td>\n",
       "      <td>Best Everyday Hot tea</td>\n",
       "      <td>I have had nothing but Red Rose growing up nea...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B001VJ0B0I</td>\n",
       "      <td>2</td>\n",
       "      <td>Read the Ingredient List</td>\n",
       "      <td>Make sure you read the list of ingredients bef...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000FFPXJ8</td>\n",
       "      <td>5</td>\n",
       "      <td>Good stuff</td>\n",
       "      <td>These oatmeal cups are extremely convenient an...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId  Rating                                            Summary  \\\n",
       "0  B001D0KG3W       5                               Italian blend coffee   \n",
       "1  B001VJ0B0I       2  Gross by-products, sugar, and food colorings--...   \n",
       "2  B0005XMOI8       5                              Best Everyday Hot tea   \n",
       "3  B001VJ0B0I       2                           Read the Ingredient List   \n",
       "4  B000FFPXJ8       5                                         Good stuff   \n",
       "\n",
       "                                                Text     Label  \n",
       "0  Great coffee - much better price when you orde...  POSITIVE  \n",
       "1  I completely agree that the ingredients for th...  NEGATIVE  \n",
       "2  I have had nothing but Red Rose growing up nea...  POSITIVE  \n",
       "3  Make sure you read the list of ingredients bef...  NEGATIVE  \n",
       "4  These oatmeal cups are extremely convenient an...  POSITIVE  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# middle 6019 reviews\n",
    "validation_data = data[70000:76019].reset_index(drop=True)\n",
    "validation_data_length = validation_data.shape[0]\n",
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6019, 5)\n",
      "(6019,)\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(validation_data.shape)\n",
    "pp.pprint(validation_data.Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003NV2IG2</td>\n",
       "      <td>2</td>\n",
       "      <td>Inconsistent Taste</td>\n",
       "      <td>I first tasted Annie Chun's seaweed three year...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0001JXBE2</td>\n",
       "      <td>4</td>\n",
       "      <td>These are *not* sproutable oats!</td>\n",
       "      <td>I have been searching for unstabilized, organi...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B003NV2IG2</td>\n",
       "      <td>2</td>\n",
       "      <td>Nothing like sushi, that's for sure</td>\n",
       "      <td>For me, these were a total bust. The texture s...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000LKTPV4</td>\n",
       "      <td>4</td>\n",
       "      <td>Organic, delicious, and fair trade. Can't ask ...</td>\n",
       "      <td>Loved it. It's maybe the best white chocolate ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B003NV2IG2</td>\n",
       "      <td>2</td>\n",
       "      <td>Mediocre for Seaweed Snacking</td>\n",
       "      <td>This is one of many varieties of seaweed snack...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId  Rating                                            Summary  \\\n",
       "0  B003NV2IG2       2                                 Inconsistent Taste   \n",
       "1  B0001JXBE2       4                   These are *not* sproutable oats!   \n",
       "2  B003NV2IG2       2                Nothing like sushi, that's for sure   \n",
       "3  B000LKTPV4       4  Organic, delicious, and fair trade. Can't ask ...   \n",
       "4  B003NV2IG2       2                      Mediocre for Seaweed Snacking   \n",
       "\n",
       "                                                Text     Label  \n",
       "0  I first tasted Annie Chun's seaweed three year...  NEGATIVE  \n",
       "1  I have been searching for unstabilized, organi...  POSITIVE  \n",
       "2  For me, these were a total bust. The texture s...  NEGATIVE  \n",
       "3  Loved it. It's maybe the best white chocolate ...  POSITIVE  \n",
       "4  This is one of many varieties of seaweed snack...  NEGATIVE  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last 6018 reviews\n",
    "test_data = data[76019:82037].reset_index(drop=True)\n",
    "test_data_length = test_data.shape[0]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6018, 5)\n",
      "(6018,)\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(test_data.shape)\n",
    "pp.pprint(test_data.Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Ineffiencies in Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning\n",
    "\n",
    "Given the volume of our data, the training speed is still pretty low and have to find ways to increment it. Since, effectively, **0 multiplied by any number is still 0**, we could find a way to disregard inputs that are **0** and only consider those with **1**. \n",
    "\n",
    "This will improve our computation time allowing us to train on even more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eficient and Low Noise SentimentalNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's tweak our network from before to model these phenomena\n",
    "class EfficientLowNoiseSentimentalNeuralNetwork(object):\n",
    "    def __init__(self, training_data, num_hidden_nodes=10, num_epochs=10, learning_rate=0.1, min_count=10,polar_cutoff=0.1):\n",
    "        # set our random number generator \n",
    "        np.random.seed(1)\n",
    "        # set our improvement parameters\n",
    "        self.min_count = min_count\n",
    "        # ToDo -cater different centers of the frquency ditribution\n",
    "        self.polar_cutoff = polar_cutoff\n",
    "        # pre-process data\n",
    "        self.pre_process_data(training_data)\n",
    "        \n",
    "        # set network paramaters\n",
    "        self.num_features = len(self.vocab)\n",
    "        self.vocab_vector = np.zeros((1, len(self.vocab)))\n",
    "        self.num_input_nodes = self.num_features\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_output_nodes = 1\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # for printing later\n",
    "        self.losses = {'train':[], 'validation':[]}\n",
    "        self.accuracy = {'train':[], 'validation':[]}\n",
    "        self.confusion_matrix = np.zeros((1,4))\n",
    "        # initialize weights\n",
    "        self.weights_i_h = np.random.randn(self.num_input_nodes, self.num_hidden_nodes)\n",
    "        self.weights_h_o = np.random.randn(self.num_hidden_nodes, self.num_output_nodes)\n",
    "         # initialize weights\n",
    "        self.bias_i_h = np.zeros(self.num_hidden_nodes)\n",
    "        self.bias_h_o = np.zeros(self.num_output_nodes)\n",
    "        # initialise the hidden layer with zeros\n",
    "        self.hidden_layer = np.zeros((self.num_output_nodes, self.num_hidden_nodes))\n",
    "        \n",
    "    def forward_backward_propagate(self, text, label):\n",
    "        ### Forward pass ###\n",
    "        # Input Layer & Hidden layer operation\n",
    "        self.hidden_layer *= 0\n",
    "        for index in text:\n",
    "            self.hidden_layer += self.weights_i_h[index]\n",
    "        self.hidden_layer += self.bias_i_h\n",
    "            \n",
    "        # Output layer\n",
    "        output_layer = self.sigmoid(self.hidden_layer.dot(self.weights_h_o) + self.bias_h_o)\n",
    "        \n",
    "        ### Backward pass ###\n",
    "        # Output error\n",
    "        output_layer_error = output_layer - self.get_target_for_label(label)\n",
    "        output_layer_delta = output_layer_error * self.sigmoid_derivative(output_layer)\n",
    "\n",
    "        # Backpropagated error - to the hidden layer\n",
    "        hidden_layer_error = output_layer_delta.dot(self.weights_h_o.T)\n",
    "        # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "        hidden_layer_delta = output_layer_error \n",
    "\n",
    "        # update the weights and bias - with grdient descent\n",
    "        self.weights_h_o -= self.hidden_layer.T.dot(output_layer_delta) * self.learning_rate \n",
    "        self.bias_h_o -= output_layer_delta[0] * self.learning_rate \n",
    "        # update only the weights and bias used in the forward pass\n",
    "        for index in text:\n",
    "            self.weights_i_h[index] -= hidden_layer_delta[0] * self.learning_rate\n",
    "        self.bias_i_h -= hidden_layer_delta[0] * self.learning_rate\n",
    "        \n",
    "        if(output_layer >= 0.5 and self.get_target_for_label(label) == 1):\n",
    "                self.correct_so_far += 1\n",
    "        elif(output_layer < 0.5 and self.get_target_for_label(label) == 0):\n",
    "                self.correct_so_far += 1\n",
    "        \n",
    "    def train(self):\n",
    "        # process data to eliminate zero's\n",
    "        training_data_text = list()\n",
    "        for review in training_data.Text:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word_to_column.keys()):\n",
    "                    indices.add(self.word_to_column[word])\n",
    "            training_data_text.append(list(indices))\n",
    "        \n",
    "        # iterate through all epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.correct_so_far = 0\n",
    "            start = time.time()\n",
    "            training_loss = 0\n",
    "            validation_loss = 0\n",
    "            training_accuracy = 0\n",
    "\n",
    "            # train over all rows of training data\n",
    "            for row in range(training_data.shape[0]):\n",
    "                # Forward and Back Propagation\n",
    "                self.forward_backward_propagate(training_data_text[row], training_data.Label[row])\n",
    "                # calculate our speed\n",
    "                elasped_time = float(time.time() - start + 0.001)\n",
    "                samples_per_second = row / float(elasped_time)\n",
    "                # calculate our accuracy\n",
    "                training_accuracy = self.correct_so_far * 100 / float(row+1)\n",
    "                # print progress of training\n",
    "                sys.stdout.write(\"\\rEpoch: \"+ str(epoch)\n",
    "                                 + \" Progress: \" + str(100 * row/float(training_data.shape[0]))[:4] + \"%\"\n",
    "                                 + \" Speed(samples/sec): \" + str(samples_per_second)[0:5] \n",
    "                                 + \" #Correct: \" + str(self.correct_so_far) \n",
    "                                 + \" #Trained: \" + str(row+1) \n",
    "                                 + \" Training Accuracy: \" + str(training_accuracy)[:4] + \"%\")\n",
    "            \n",
    "            self.accuracy[\"train\"].append(training_accuracy) \n",
    "                \n",
    "            training_loss = self.run(training_data[0:7000])\n",
    "            validation_loss = self.run(validation_data,  mode=\"validate\")\n",
    "            self.losses[\"train\"].append(training_loss)\n",
    "            self.losses[\"validation\"].append(validation_loss)      \n",
    "            print(\"\")\n",
    "            \n",
    "    def run(self, input_data, mode=\"train\"):\n",
    "        # total losses for sample\n",
    "        val_correct_so_far = 0\n",
    "        val_accuracy = 0\n",
    "        loss = 0\n",
    "        # iterate through all training samples\n",
    "        for row in range(0, input_data.shape[0]):\n",
    "            # get prediction\n",
    "            pred = self.predict(input_data.Text[row])\n",
    "            # calculate the loss\n",
    "            loss += np.mean((pred - self.get_target_for_label(input_data.Label[row]))**2)\n",
    "            # Calculate our accuracy\n",
    "            if(mode is \"validate\"):\n",
    "                #calculate the accuracy\n",
    "                if(pred >= 0.5 and self.get_target_for_label(input_data.Label[row]) == 1):\n",
    "                    val_correct_so_far += 1\n",
    "                elif(pred < 0.5 and self.get_target_for_label(input_data.Label[row]) == 0):\n",
    "                    val_correct_so_far += 1\n",
    "        if(mode is \"validate\"):\n",
    "            val_accuracy = val_correct_so_far * 100 / float(input_data.shape[0])\n",
    "            self.accuracy[\"validation\"].append(val_accuracy) \n",
    "        return loss/float(input_data.shape[0]) \n",
    "            \n",
    "            \n",
    "    def test(self, test_data):\n",
    "        # How many predictions are correct out of total training\n",
    "        correct = 0\n",
    "        # Reset cnfusion matrix \n",
    "        self.confusion_matrix = np.zeros((1,4))\n",
    "        # start time of one epoch\n",
    "        start = time.time()\n",
    "        # iterate through all training samples\n",
    "        for i in range(0, test_data.shape[0]):\n",
    "            # get prediction\n",
    "            pred = self.predict(test_data.Text[i])\n",
    "            # count how many we validate as correct\n",
    "            if(pred >= 0.5 and self.get_target_for_label(test_data.Label[i]) == 1):\n",
    "                correct += 1\n",
    "            elif(pred < 0.5 and self.get_target_for_label(test_data.Label[i]) == 0):\n",
    "                correct += 1\n",
    "            # create confusion matrix    \n",
    "            self.confusion_matrix += self.calculate_confusion_matrix(np.rint(pred), self.get_target_for_label(test_data.Label[i]))\n",
    "            # calculate our sampling rate\n",
    "            reviews_per_second = i / float(time.time() - start + 0.001)\n",
    "            # print out the validation metrics\n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(test_data.shape[0]))[:4] + \"%\"\n",
    "                             + \" Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \n",
    "                             + \" #Correct:\" + str(correct) \n",
    "                             + \" #Tested:\" + str(i+1) \n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "            \n",
    "    def predict(self, text):\n",
    "        # prepare the data\n",
    "        indices = set()\n",
    "        for word in text.split(\" \"):\n",
    "            if(word in self.word_to_column.keys()):\n",
    "                indices.add(self.word_to_column[word])\n",
    "        \n",
    "        ### Forward pass ###\n",
    "        # Input Layer & Hidden layer operation\n",
    "        self.hidden_layer *= 0\n",
    "        for index in indices:\n",
    "            self.hidden_layer += self.weights_i_h[index]\n",
    "        self.hidden_layer += self.bias_i_h\n",
    "\n",
    "        # output layer\n",
    "        output_layer = self.sigmoid(self.hidden_layer.dot(self.weights_h_o) + self.bias_h_o)\n",
    "        \n",
    "        return output_layer.flatten()\n",
    "    \n",
    "    \n",
    "    def visualise_training(self):\n",
    "        plt.figure(1)\n",
    "        plt.title('Training, LR: ' + str(self.learning_rate) + ' HLU: ' + str(self.num_hidden_nodes))\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(self.losses['train'], label='Training loss')\n",
    "        plt.plot(self.losses['validation'], label='Validation loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        file_path = \"training/plot_loss_lr\" + str(self.learning_rate) + \"epoch_\" + str(self.num_epochs)+ \"hlu_\" + str(self.num_hidden_nodes) + \".png\"\n",
    "        self.save_plot(file_path)\n",
    "        \n",
    "        plt.figure(2)\n",
    "        plt.title('Training, LR: ' + str(self.learning_rate) + ' HLU: ' + str(self.num_hidden_nodes))\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.plot(self.accuracy['train'], label='Training Accuracy')\n",
    "        plt.plot(self.accuracy['validation'], label='Validation Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        file_path = \"training/plot_acc_lr\" + str(self.learning_rate) + \"epoch_\" + str(self.num_epochs) + \"hlu_\" + str(self.num_hidden_nodes) + \".png\"\n",
    "        self.save_plot(file_path)\n",
    "        \n",
    "    def save_plot(self, file_path):\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        plt.savefig(file_path, bbox_inches='tight')\n",
    "                      \n",
    "    def get_confusion_matrix(self):\n",
    "        hyper_param_dict = {'EP':self.num_epochs, 'LR':self.learning_rate, 'HLU': self.num_hidden_nodes}\n",
    "        df = pd.DataFrame(data=[hyper_param_dict], columns=['EP', 'LR', 'HLU'])\n",
    "        \n",
    "        tmp_df = pd.DataFrame(data=self.confusion_matrix, columns=['TP','FP', 'TN', 'FN'])\n",
    "        # combine the rows, not columns i.e axis=1\n",
    "        df = pd.concat([df, tmp_df], axis=1)\n",
    "    \n",
    "        TP = self.confusion_matrix[0][0]\n",
    "        FP = self.confusion_matrix[0][1]\n",
    "        TN = self.confusion_matrix[0][2]\n",
    "        FN = self.confusion_matrix[0][3]\n",
    "\n",
    "        recall = TP/(TP + FN)\n",
    "        precision = TP/(TP + FP)\n",
    "        f_one_score = (2*recall*precision)/(recall + precision)\n",
    "        mcc_score = ((TP * TN) - (FP*FN))/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "        test_accuracy = (TP + TN)/(TP +TN +FP +FN)\n",
    "        \n",
    "        df['RECALL'] = recall *100\n",
    "        df['PRECISION'] = precision *100\n",
    "        df['F1-S'] = f_one_score *100\n",
    "        df['MCC-S'] = f_one_score *100\n",
    "        df['TE-ACC'] = test_accuracy *100\n",
    "        df['TR-ACC'] = None if not self.accuracy['train'] else self.accuracy['train'][self.num_epochs -1]\n",
    "        df['VA-ACC'] = None if not self.accuracy['validation'] else self.accuracy['validation'][self.num_epochs -1]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def pre_process_data(self, training_data):\n",
    "        # frequency of words in positive reviews\n",
    "        positive_counts = Counter()\n",
    "        # frequency of words in negative reviews\n",
    "        negative_counts = Counter()\n",
    "        # frequency of words in all reviews\n",
    "        total_counts = Counter()\n",
    "        # affinity of words for being in positive/negative reviews\n",
    "        positive_negative_ratios = Counter()\n",
    "        \n",
    "        # get the counts\n",
    "        for i in range(training_data.shape[0]):\n",
    "            if(training_data.Label[i] == 'POSITIVE'):\n",
    "                for word in training_data.Text[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            if(training_data.Label[i] == 'NEGATIVE'):\n",
    "                for word in training_data.Text[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "        # calculate positive-negative affinity\n",
    "        for term, count in list(total_counts.most_common()):\n",
    "            # consider only words that appear more than 50 times\n",
    "            if(count >= 50):\n",
    "                positive_negative_ratio = float(positive_counts[term]) / float(negative_counts[term]+1)\n",
    "                positive_negative_ratios[term] = positive_negative_ratio\n",
    "\n",
    "        for word, ratio in positive_negative_ratios.most_common():\n",
    "            # normalise the ratio\n",
    "            if(ratio > 1):\n",
    "                positive_negative_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                positive_negative_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "        \n",
    "        self.vocab = set()\n",
    "        \n",
    "        for review in training_data.Text:\n",
    "            for word in review.split(\" \"):\n",
    "                # eliminate low freqeuncy words\n",
    "                if(total_counts[word] > self.min_count):\n",
    "                    if(word in positive_negative_ratios.keys()):\n",
    "                        # eliminate words with very high frequency on both sides of the spectrum\n",
    "                        if((positive_negative_ratios[word] >= self.polar_cutoff) or (positive_negative_ratios[word] <= -self.polar_cutoff)):\n",
    "                            self.vocab.add(word)\n",
    "                    else:    \n",
    "                        self.vocab.add(word)\n",
    "        # convert to list so that we can access using indices        \n",
    "        self.vocab = list(self.vocab)\n",
    "        # create our vocab to column index mapping\n",
    "        self.word_to_column = {}\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            self.word_to_column[word] = i\n",
    "    \n",
    "    def calculate_confusion_matrix(self, y_predicted, y_actual):\n",
    "        #True/False Positive and True/False Negative \n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "\n",
    "        if y_actual==y_predicted==1:\n",
    "               TP += 1\n",
    "        if y_predicted==1 and y_actual!=y_predicted:\n",
    "               FP += 1\n",
    "        if y_actual==y_predicted==0:\n",
    "               TN += 1\n",
    "        if y_predicted==0 and y_actual!=y_predicted:\n",
    "               FN += 1\n",
    "\n",
    "        return np.array((TP, FP, TN, FN))\n",
    "\n",
    "    def get_target_for_label(self, label):\n",
    "        if(label == 'POSITIVE'):\n",
    "            return 1\n",
    "        elif(label == 'NEGATIVE'):\n",
    "            return 0\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self,x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnetEffLN = EfficientLowNoiseSentimentalNeuralNetwork(training_data, num_hidden_nodes=10, num_epochs=20, learning_rate=0.01, min_count=10, polar_cutoff=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you check for any biasness before training. We should have a **50-50** chance for both `POSITIVE` and `NEGATIVE` reviews. This is because the network hasn't even trained yet and what it is outputing is basically guesswork.\n",
    "\n",
    "**NB** In general starting from a non-biased point like this is very desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):4550. #Correct:2968 #Tested:6018 Testing Accuracy:49.3%"
     ]
    }
   ],
   "source": [
    "nnetEffLN.test(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the confusion matrix for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EP</th>\n",
       "      <th>LR</th>\n",
       "      <th>HLU</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>F1-S</th>\n",
       "      <th>MCC-S</th>\n",
       "      <th>TE-ACC</th>\n",
       "      <th>TR-ACC</th>\n",
       "      <th>VA-ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>42.771685</td>\n",
       "      <td>49.216061</td>\n",
       "      <td>45.768137</td>\n",
       "      <td>45.768137</td>\n",
       "      <td>49.318711</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EP    LR  HLU      TP      FP      TN      FN     RECALL  PRECISION  \\\n",
       "0  20  0.01   10  1287.0  1328.0  1681.0  1722.0  42.771685  49.216061   \n",
       "\n",
       "        F1-S      MCC-S     TE-ACC TR-ACC VA-ACC  \n",
       "0  45.768137  45.768137  49.318711   None   None  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnetEffLN.get_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Progress: 77.5% Speed(samples/sec): 4712. #Correct: 39199 #Trained: 54296 Training Accuracy: 72.1%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-395dec1d5228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnnetEffLN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-173d6b5dece2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[1;31m# Forward and Back Propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_backward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[1;31m# calculate our speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0melasped_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-173d6b5dece2>\u001b[0m in \u001b[0;36mforward_backward_propagate\u001b[0;34m(self, text, label)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[1;31m# Backpropagated error - to the hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mhidden_layer_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_layer_delta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_h_o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[1;31m# hidden layer gradients - no nonlinearity so it's the same as the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mhidden_layer_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_layer_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nnetEffLN.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network has already been trained, and obviously we now don't expect a **50-50** performance but something much better if indeed the newtork has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):4767. #Correct:4553 #Tested:6018 Testing Accuracy:75.6%"
     ]
    }
   ],
   "source": [
    "nnetEffLN.test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load our saved data\n",
    "all_data = pd.read_csv('dataset/pre_processed_reviews.csv', encoding='latin-1')\n",
    "all_data = all_data.loc[(all_data.Label == \"POSITIVE\")].sample(frac=0.5).reset_index(drop=True)\n",
    "nnetEffLN.test(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the confusion matrix for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnetEffLN.get_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A graph of validation/training loss against the no. of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nnetEffLN.visualise_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict sentiment of unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnetEffLN.predict(\"This product sucks!! I bought it and it proved disgusting. Do not buy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnetEffLN.predict(\"I enjoyed it. It is a very awesome product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If satisfied with the results, update our training data over time. We will use this for analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix_to_csv(df, csvFilePath, sep=\",\"):\n",
    "    if not os.path.isfile(csvFilePath):\n",
    "        df.to_csv(csvFilePath, mode='a', index=False, sep=sep)\n",
    "    else:\n",
    "        df.to_csv(csvFilePath, mode='a', index=False, sep=sep, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update training data to disk\n",
    "csv_train_filepath = \"training/XXXsentiment_cf_matrix_lr.csv\"\n",
    "ensure_dir(csv_train_filepath)\n",
    "df = nnetEffLN.get_confusion_matrix()\n",
    "save_confusion_matrix_to_csv(df, csv_train_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show training data and progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load our saved data\n",
    "csv_train_filepath = \"training/sentiment_cf_matrix_lr.csv\"\n",
    "training_metrics = pd.read_csv(csv_train_filepath, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show first 10 results\n",
    "training_metrics.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing of short codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- EP: No. of Epochs  \n",
    "- LR: Learning Rate\n",
    "- HLU: No. of Hiddel Layer Units\n",
    "- TP: True Positive\n",
    "- FP - False Positive\n",
    "- TN: - True Negative\n",
    "- FN: - False Negative\n",
    "- F1-S : - F1-Score\n",
    "- MCC-S: - Matthews Correlation Coefficient\n",
    "- TE-ACC: Testing Accuracy for last Epoch\n",
    "- TR-ACC: Training Accuracy for last Epoch\n",
    "- VA-ACC: Validation Accuracy for last Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tabulation of training results for different hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='training/tables/results_final.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "model_trained_filepath = 'models/se_model_lr0.001epoch_100hlu_30.sav'\n",
    "ensure_dir(model_trained_filepath)\n",
    "cPickle.dump(nnetEffLN, open(model_trained_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "model_trained_filepath = 'models/se_model_lr0.01epoch_20hlu_10.sav'\n",
    "loaded_model = cPickle.load(open(model_trained_filepath, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict sentiment of unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the results for loaded model are same as for the pre-saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_model.predict(\"This product sucks!! I bought it and it proved disgusting. Do not buy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_model.predict(\"I enjoyed it. It is a very awesome product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_model.predict(\"I hate how this product was badly packaged. It was delivered while damaged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy saved trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the model can be deployed and distributed as a service. It can be deployed as a web application, mobile app, smart personal asistants such as `Google Now`,`Google Home`, `Amazon Echo` e.t.c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
